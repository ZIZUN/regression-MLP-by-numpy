{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Numpy: Tutorial\n",
    "## 1.1 dot, einsum\n",
    "### dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7 10]\n",
      " [15 22]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2],[3,4]])\n",
    "b = a\n",
    "\n",
    "print(np.dot(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "a=4\n",
    "\n",
    "print(np.dot(a,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(24).reshape((2,3,4))\n",
    "b = np.arange(24).reshape((2,4,3))\n",
    "print(np.dot(a,b).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(6).reshape((3,2,1))\n",
    "b = np.arange(6).reshape((3,1,2))\n",
    "\n",
    "print(np.dot(a,b).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(16).reshape((2,2,4))\n",
    "b = np.arange(16).reshape((2,4,2))\n",
    "print(np.dot(a,b).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n"
     ]
    }
   ],
   "source": [
    "# Transpose\n",
    "\n",
    "A = np.array([[1,2,3], [4,5,6]])\n",
    "R = np.einsum(\"ij->ji\", A)\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "# diagonal, Trace\n",
    "\n",
    "A = np.eye(10)\n",
    "print(A)\n",
    "diag = np.einsum('ii->i', A)\n",
    "trace =np.einsum('ii->', A)\n",
    "print(diag)\n",
    "print(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "# matrix sum to scalar\n",
    "\n",
    "A = np.array([[1,2,3], [4,5,6]])\n",
    "R = np.einsum(\"ij->\", A)\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6 15]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "# matrix column or row sum (to vector)\n",
    "\n",
    "A = np.array([[1,2,3], [4,5,6]])\n",
    "row_sum = np.einsum(\"ij->i\", A)\n",
    "col_sum =np.einsum(\"ij->j\", A)\n",
    "print(row_sum)\n",
    "print(col_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10101\n",
      "[[    -1    -10   -100]\n",
      " [   -10   -100  -1000]\n",
      " [  -100  -1000 -10000]]\n"
     ]
    }
   ],
   "source": [
    "# Dot Product, Outer product of two vectors\n",
    "\n",
    "x = np.array([-1, -10, -100])\n",
    "y = np.array([1, 10, 100])\n",
    "dot = np.einsum('i,i->', x, y ) # dot product\n",
    "outer = np.einsum('i,j->ij', x,y) # outer product\n",
    "print(dot)\n",
    "print(outer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    -1   -100 -10000]\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "# Hadamard(element-wise) product of vector or matrix\n",
    "\n",
    "x = np.array([-1, -10, -100])\n",
    "y = np.array([1, 10, 100])\n",
    "elemwise_vec = np.einsum('i,i->i', x, y)\n",
    "print(elemwise_vec)\n",
    "A = np.arange(6).reshape((2, 3))\n",
    "B = np.arange(6).reshape((2, 3))\n",
    "elemwise_mat = np.einsum('ij,ij->', A, B)\n",
    "print(elemwise_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-321 -654]\n",
      "[[14 32]\n",
      " [32 77]]\n",
      "[[[0.40298129 0.59043898]\n",
      "  [0.0739351  0.10832802]]\n",
      "\n",
      " [[0.24378779 0.07427415]\n",
      "  [0.4942785  0.15059046]]\n",
      "\n",
      " [[0.06461978 0.14414622]\n",
      "  [0.24197069 0.53975978]]]\n",
      "(3, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "# Matrix-Vector multiplication\n",
    "\n",
    "A = np.array([[1,2,3], [4,5,6]])\n",
    "x = np.array([-1, -10, -100])\n",
    "b = np.einsum('ij,j->i', A, x)\n",
    "print(b)\n",
    "\n",
    "## Matrix-Matrix Multiplication\n",
    "A = np.array([[1,2,3], [4,5,6]])\n",
    "b = A.transpose()\n",
    "R = np.einsum('ik,kj->ij', A, b)\n",
    "print(R)\n",
    "\n",
    "## Batched Matrix Multiplication\n",
    "A = np.random.random(size=(3,2,1))\n",
    "B = np.random.random(size=(3,1,2))\n",
    "\n",
    "R = np.einsum('bik,bkj->bij',A, B)\n",
    "print(R)\n",
    "print(R.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-18.550454732519377\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2,3])\n",
    "y = np.array([-1,-2,-3])\n",
    "A = np.random.random(size=(3, 3))\n",
    "\n",
    "r = np.einsum('i,ij,j->', x, A, y)\n",
    "\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 quiz풀이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  4]\n",
      " [ 5  6  7  8  9]\n",
      " [10 11 12 13 14]\n",
      " [15 16 17 18 19]\n",
      " [20 21 22 23 24]]\n",
      "[[ 5  6  7  8  9]\n",
      " [ 0  1  2  3  4]\n",
      " [10 11 12 13 14]\n",
      " [15 16 17 18 19]\n",
      " [20 21 22 23 24]]\n"
     ]
    }
   ],
   "source": [
    "# 72. How to swap two rows of an array? (★★★)\n",
    "\n",
    "A = np.arange(25).reshape(5,5)\n",
    "print(A)\n",
    "A[[0,1]] = A[[1,0]]\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 15,  54, 111])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 69. How to get the diagonal of a dot product? (★★★)\n",
    "\n",
    "A = np.arange(9).reshape((3,3))\n",
    "B = np.arange(9).reshape((3,3))\n",
    "\n",
    "np.einsum(\"ij,ji->i\", A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2 3 4 4 6]\n"
     ]
    }
   ],
   "source": [
    "# 74. Given an array C that is a bincount, how to produce an array A such that np.bincount(A) == C? (★★★)\n",
    "\n",
    "C = np.bincount([1,1,2,3,4,4,6])\n",
    "A = np.repeat(np.arange(len(C)), C)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 2 4 5 8 8 7 0 0 6 6 6 5 1 9 4 0 1 6 1 8 8 6 1 9 9 7 9 2 9 4 9 4 7 6 1 5\n",
      " 4 3 5 3 7 9 0 0 4 7 0 3 9]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# 83. How to find the most frequent value in an array? (★★★)\n",
    "Z = np.random.randint(0,10,50)\n",
    "print(Z)\n",
    "print(np.bincount(Z).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9989 9990 9991 9992 9993 9994 9995 9996 9997 9998 9999]\n"
     ]
    }
   ],
   "source": [
    "# 89. How to get the n largest values of an array (★★★)\n",
    "\n",
    "Z = np.arange(10000)\n",
    "np.random.shuffle(Z)\n",
    "n = 11\n",
    "\n",
    "print (Z[np.argsort(Z)[-n:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 선형방정식 풀기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.5  0.5 -0.5]\n",
      " [ 0.   1.   2. ]]\n",
      "[[ 3.60000000e-01  1.60000000e-01 -4.00000000e-02]\n",
      " [ 2.40000000e-01  1.40000000e-01  4.00000000e-02]\n",
      " [ 1.20000000e-01  1.20000000e-01  1.20000000e-01]\n",
      " [-4.16333634e-17  1.00000000e-01  2.00000000e-01]\n",
      " [-1.20000000e-01  8.00000000e-02  2.80000000e-01]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import inv\n",
    "# Ax = b 풀기\n",
    "\n",
    "A = np.arange(4).reshape((2,2))\n",
    "b = np.arange(6).reshape((2,3))\n",
    "\n",
    "print(np.dot(np.linalg.inv(A),b))\n",
    "\n",
    "# pseudo inverse 이용\n",
    "\n",
    "A = np.arange(10).reshape((2,5))\n",
    "b = np.arange(6).reshape((2,3))\n",
    "\n",
    "print(np.dot(np.linalg.pinv(A),b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 만들고 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100)\n",
      "(1,)\n",
      "(10000, 100)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "\n",
    "## 데이터셋 만들고 저장\n",
    "\n",
    "w = 20*np.random.rand(1,100)-10 \n",
    "b = 20*np.random.rand(1)-10\n",
    "\n",
    "x = 20*np.random.rand(10000,100,1)-10\n",
    "y = np.matmul(w,x)\n",
    "\n",
    "x=np.squeeze(x, axis=2)\n",
    "y=np.squeeze(y, axis=2)+b\n",
    "\n",
    "print(w.shape)\n",
    "print(b.shape)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "\n",
    "# pickle 이용해 저장\n",
    "\n",
    "with open(\"linear/myrandomdataset_w.pickle\",\"wb\") as fw:\n",
    "    pickle.dump(w, fw)\n",
    "with open(\"linear/myrandomdataset_b.pickle\",\"wb\") as fw:\n",
    "    pickle.dump(b, fw)\n",
    "with open(\"linear/myrandomdataset_x.pickle\",\"wb\") as fw:\n",
    "    pickle.dump(x, fw)\n",
    "with open(\"linear/myrandomdataset_y.pickle\",\"wb\") as fw:\n",
    "    pickle.dump(y, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(y.shape)\n",
    "plt.figure\n",
    "plt.hist(y, 444)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "\n",
    "#배치 뽑는 함수\n",
    "\n",
    "def get_mini_batches(X, y, batch_size): \n",
    "    random_idxs = np.random.choice(len(y), len(y), replace=False)\n",
    "    X_shuffled = X[random_idxs,:]\n",
    "    y_shuffled = y[random_idxs]\n",
    "    mini_batches = [(X_shuffled[i:i+batch_size,:], y_shuffled[i:i+batch_size]) for\n",
    "                   i in range(0, len(y), batch_size)]\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle 이용해 읽기\n",
    "\n",
    "with open(\"linear/myrandomdataset_w.pickle\",\"rb\") as fr:\n",
    "    w_ = pickle.load(fr)\n",
    "with open(\"linear/myrandomdataset_b.pickle\",\"rb\") as fr:\n",
    "    b_ = pickle.load(fr)\n",
    "with open(\"linear/myrandomdataset_x.pickle\",\"rb\") as fr:\n",
    "    x_ = pickle.load(fr)\n",
    "with open(\"linear/myrandomdataset_y.pickle\",\"rb\") as fr:\n",
    "    y_ = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442, 1)\n"
     ]
    }
   ],
   "source": [
    "a_=diabetes['data']\n",
    "b_ =np.expand_dims(diabetes['target'], axis=1)\n",
    "\n",
    "print(a_.shape)\n",
    "print(b_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 100)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "d_size = 1000 #deafult   N=1000\n",
    "if(x_.shape[0]==10000): # N의 사이즈에 따른 설정 (1000, 10000, 100000 중 택 1)\n",
    "    d_size=10000\n",
    "elif(x_.shape[0]==100000):\n",
    "    d_size=100000\n",
    "elif(x_.shape[0]==442):\n",
    "    d_size=442\n",
    "    \n",
    "# 학습용데이터\n",
    "train_x = x_[0:int(d_size*0.85)]\n",
    "train_y = y_[0:int(d_size*0.85)]\n",
    "\n",
    "# dev 데이터\n",
    "dev_x = x_[int(d_size*0.85):int(d_size*0.9)]\n",
    "dev_y = y_[int(d_size*0.85):int(d_size*0.9)]\n",
    "\n",
    "# test 데이터\n",
    "test_x = x_[int(d_size*0.9):d_size]\n",
    "test_y = y_[int(d_size*0.9):d_size]\n",
    "\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 100)\n",
      "(300, 1)\n"
     ]
    }
   ],
   "source": [
    "#파라미터 설정\n",
    "\n",
    "epochs = 100 # 최대 에폭\n",
    "learning_rate = 0.01\n",
    "batch_size = 300\n",
    "dimension = 100\n",
    "max_data = train_x.shape[0]\n",
    "\n",
    "W = np.zeros((dimension,1))\n",
    "b = 0\n",
    "\n",
    "train_batch = get_mini_batches(train_x, train_y, batch_size)[0] \n",
    "\n",
    "train_batch_x= train_batch[0]\n",
    "train_batch_y= train_batch[1]\n",
    "\n",
    "print(train_batch_x.shape)\n",
    "print(train_batch_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "[Train Loss]  0.830723\n",
      "[Dev Loss]  0.051400\n",
      "[Test Loss]  0.105754\n",
      "w squared error : 0.001198 , b squared error : 67230.262372\n",
      "\n",
      "Epoch  2\n",
      "[Train Loss]  0.404180\n",
      "[Dev Loss]  0.025010\n",
      "[Test Loss]  0.051898\n",
      "w squared error : 0.000574 , b squared error : 67553.424400\n",
      "\n",
      "Epoch  3\n",
      "[Train Loss]  0.198504\n",
      "[Dev Loss]  0.012282\n",
      "[Test Loss]  0.025494\n",
      "w squared error : 0.000282 , b squared error : 67779.833377\n",
      "\n",
      "Epoch  4\n",
      "[Train Loss]  0.097497\n",
      "[Dev Loss]  0.006033\n",
      "[Test Loss]  0.012521\n",
      "w squared error : 0.000138 , b squared error : 67938.507506\n",
      "\n",
      "Epoch  5\n",
      "[Train Loss]  0.047886\n",
      "[Dev Loss]  0.002963\n",
      "[Test Loss]  0.006150\n",
      "w squared error : 0.000068 , b squared error : 68049.710542\n",
      "\n",
      "Epoch  6\n",
      "[Train Loss]  0.023520\n",
      "[Dev Loss]  0.001455\n",
      "[Test Loss]  0.003021\n",
      "w squared error : 0.000033 , b squared error : 68127.644567\n",
      "\n",
      "Epoch  7\n",
      "[Train Loss]  0.011552\n",
      "[Dev Loss]  0.000715\n",
      "[Test Loss]  0.001484\n",
      "w squared error : 0.000016 , b squared error : 68182.262791\n",
      "\n",
      "Epoch  8\n",
      "[Train Loss]  0.005674\n",
      "[Dev Loss]  0.000351\n",
      "[Test Loss]  0.000729\n",
      "w squared error : 0.000008 , b squared error : 68220.540685\n",
      "\n",
      "Epoch  9\n",
      "[Train Loss]  0.002787\n",
      "[Dev Loss]  0.000172\n",
      "[Test Loss]  0.000358\n",
      "w squared error : 0.000004 , b squared error : 68247.366844\n",
      "\n",
      "Epoch  10\n",
      "[Train Loss]  0.001369\n",
      "[Dev Loss]  0.000085\n",
      "[Test Loss]  0.000176\n",
      "w squared error : 0.000002 , b squared error : 68266.167325\n",
      "\n",
      "Epoch  11\n",
      "[Train Loss]  0.000672\n",
      "[Dev Loss]  0.000042\n",
      "[Test Loss]  0.000086\n",
      "w squared error : 0.000001 , b squared error : 68279.343199\n",
      "\n",
      "Epoch  12\n",
      "[Train Loss]  0.000330\n",
      "[Dev Loss]  0.000020\n",
      "[Test Loss]  0.000042\n",
      "w squared error : 0.000000 , b squared error : 68288.577199\n",
      "\n",
      "Epoch  13\n",
      "[Train Loss]  0.000162\n",
      "[Dev Loss]  0.000010\n",
      "[Test Loss]  0.000021\n",
      "w squared error : 0.000000 , b squared error : 68295.048631\n",
      "\n",
      "Epoch  14\n",
      "[Train Loss]  0.000080\n",
      "[Dev Loss]  0.000005\n",
      "[Test Loss]  0.000010\n",
      "w squared error : 0.000000 , b squared error : 68299.583981\n",
      "\n",
      "Epoch  15\n",
      "[Train Loss]  0.000039\n",
      "[Dev Loss]  0.000002\n",
      "[Test Loss]  0.000005\n",
      "w squared error : 0.000000 , b squared error : 68302.762474\n",
      "\n",
      "Epoch  16\n",
      "[Train Loss]  0.000019\n",
      "[Dev Loss]  0.000001\n",
      "[Test Loss]  0.000002\n",
      "w squared error : 0.000000 , b squared error : 68304.990046\n",
      "\n",
      "Epoch  17\n",
      "[Train Loss]  0.000009\n",
      "[Dev Loss]  0.000001\n",
      "[Test Loss]  0.000001\n",
      "w squared error : 0.000000 , b squared error : 68306.551187\n",
      "\n",
      "Epoch  18\n",
      "[Train Loss]  0.000005\n",
      "[Dev Loss]  0.000000\n",
      "[Test Loss]  0.000001\n",
      "w squared error : 0.000000 , b squared error : 68307.645276\n",
      "\n",
      "Epoch  19\n",
      "[Train Loss]  0.000002\n",
      "[Dev Loss]  0.000000\n",
      "[Test Loss]  0.000000\n",
      "w squared error : 0.000000 , b squared error : 68308.412043\n",
      "\n",
      "Epoch  20\n",
      "[Train Loss]  0.000001\n",
      "[Dev Loss]  0.000000\n",
      "[Test Loss]  0.000000\n",
      "w squared error : 0.000000 , b squared error : 68308.949413\n",
      "\n",
      "Epoch  21\n",
      "[Train Loss]  0.000001\n",
      "[Dev Loss]  0.000000\n",
      "[Test Loss]  0.000000\n",
      "w squared error : 0.000000 , b squared error : 68309.326016\n",
      "\n",
      "Epoch  22\n",
      "[Train Loss]  0.000000\n",
      "[Dev Loss]  0.000000\n",
      "[Test Loss]  0.000000\n",
      "w squared error : 0.000000 , b squared error : 68309.589950\n",
      "\n",
      "Epoch  23\n",
      "[Train Loss]  0.000000\n",
      "[Dev Loss]  0.000000\n",
      "[Test Loss]  0.000000\n",
      "w squared error : 0.000000 , b squared error : 68309.774922\n",
      "\n",
      "Epoch  24\n",
      "[Train Loss]  0.000000\n",
      "[Dev Loss]  0.000000\n",
      "[Test Loss]  0.000000\n",
      "w squared error : 0.000000 , b squared error : 68309.904554\n",
      "\n",
      "Epoch  25\n",
      "[Train Loss]  0.000000\n",
      "[Dev Loss]  0.000000\n",
      "[Test Loss]  0.000000\n",
      "w squared error : 0.000000 , b squared error : 68309.995405\n",
      "\n",
      "Early Stopping !\n"
     ]
    }
   ],
   "source": [
    "# train, test\n",
    "\n",
    "dev_temp = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    for j in range(int(max_data/batch_size)):    \n",
    "        gradient_w =  np.dot(np.transpose(train_batch_x), (np.dot(train_batch_x,W) + b - train_batch_y ))* 2 / batch_size\n",
    "        gradient_b =  np.sum((np.dot(train_batch_x,W) + b - train_batch_y )* 2) / batch_size\n",
    "\n",
    "        W -= learning_rate * gradient_w\n",
    "        b -= learning_rate * gradient_b\n",
    "    \n",
    "        hypothesis = np.dot(train_batch_x,W) + b\n",
    "        Loss = np.sum((hypothesis - train_batch_y) ** 2) / batch_size\n",
    "\n",
    "    train_hypothesis = np.dot(train_x,W) + b\n",
    "    train_Loss = np.sum((train_hypothesis - train_y) ** 2) / d_size*0.1\n",
    "        \n",
    "    dev_hypothesis = np.dot(dev_x,W) + b\n",
    "    dev_Loss = np.sum((dev_hypothesis - dev_y) ** 2) / d_size*0.1\n",
    "    \n",
    "    test_hypothesis = np.dot(test_x,W) + b\n",
    "    test_Loss = np.sum((test_hypothesis - test_y) ** 2) / d_size*0.1\n",
    "    \n",
    "    w_squared_error = np.sum((W-np.transpose(w_))** 2) / dimension\n",
    "    b_squared_error = np.sum(abs(b-b_))\n",
    "    \n",
    "    print('Epoch  {:d}\\n[Train Loss]  {:f}'.format(i+1, train_Loss))\n",
    "    print('[Dev Loss]  {:f}'.format(np.sum(dev_Loss)))\n",
    "    print('[Test Loss]  {:f}'.format(np.sum(test_Loss)))\n",
    "    print('w squared error : {:f} , b squared error : {:f}'.format(w_squared_error, b_squared_error))\n",
    "    print()\n",
    "\n",
    "    if(dev_temp != 0 and (dev_temp - dev_Loss<0 or dev_temp - dev_Loss<0.00000001)):\n",
    "        print(\"Early Stopping !\")\n",
    "        break\n",
    "\n",
    "    dev_temp = test_Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "X, y = datasets.fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =np.expand_dims(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(y)\n",
    "y= enc.transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 섞어주기 위해 셔플\n",
    "\n",
    "tmp = [[a,b] for a, b in zip(X, y)]\n",
    "import random\n",
    "\n",
    "random.shuffle(tmp)\n",
    "\n",
    "X = [n[0] for n in tmp]\n",
    "y = [n[1] for n in tmp]\n",
    "\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_size = 70000 \n",
    "\n",
    "\n",
    "# 학습용데이터\n",
    "train_x = X[0:int(d_size*0.85)]\n",
    "train_y = y[0:int(d_size*0.85)]\n",
    "\n",
    "# dev 데이터\n",
    "dev_x = X[int(d_size*0.85):int(d_size*0.9)]\n",
    "dev_y = y[int(d_size*0.85):int(d_size*0.9)]\n",
    "\n",
    "# test 데이터\n",
    "test_x = X[int(d_size*0.9):d_size]\n",
    "test_y = y[int(d_size*0.9):d_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 784)\n",
      "(2000, 10)\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25b4f2de668>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADbpJREFUeJzt3X+IVXUax/HPs+UW/aKGyGTS1a1YimBrG2JJWVrDsgh/VEYW4bKxU1ixwgYb9YdBFLWsbQtFYGWNUVpRrRJbaRab1SJaSFlqRbg5OuiGgRqRms/+Mcdlsjnfc3+ce8/V5/2CuD+ee+55uvWZ77n3/PiauwtAPD+pugEA1SD8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCOrKdKzMzDicEWszdrZbXNTXym9lkM9toZp+b2R3NvBeA9rJGj+03syMkfSppkqR+SaslzXT3TxLLMPIDLdaOkf8CSZ+7+xfuvkfSYklTm3g/AG3UTPi7JW0e8rg/e+4HzKzXzNaY2Zom1gWgZM384DfcpsWPNuvdfb6k+RKb/UAnaWbk75c0esjj0yRtba4dAO3STPhXSzrTzMaZ2U8lXStpaTltAWi1hjf73X2fmd0q6XVJR0ha4O4fl9YZgJZqeFdfQyvjOz/Qcm05yAfAoYvwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaDaOkU3hnf00Ucn68uWLUvWzzjjjNzaqaeemlzWLH2h12av7vzkk0/m1mbPnp1c9rvvvmtq3Uhj5AeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoJqapdfMNknaJel7Sfvcvafg9czSO4wRI0Yk69ddd12yftZZZ5XZTl3OP//8ZH3ixIm5tSuvvDK57JIlSxrqKbpaZ+kt4yCf37r7VyW8D4A2YrMfCKrZ8LukZWb2vpn1ltEQgPZodrN/vLtvNbNTJC03sw3u/vbQF2R/FPjDAHSYpkZ+d9+a3W6X9LKkC4Z5zXx37yn6MRBAezUcfjM71syOP3Bf0iWS1pXVGIDWamazf6Skl7NTQo+U9Ky7v1ZKVwBaruHwu/sXkn5ZYi9h7d27N1nv6+trUyf16+7uTtY3bNiQW+vq6iq7HdSBXX1AUIQfCIrwA0ERfiAowg8ERfiBoLh0N5qyZcuWZP3bb7/NrY0fPz65bOqy32geIz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBMV+frRU0RTgqA4jPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ExX5+tNTAwEDVLSAHIz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBFW4n9/MFki6QtJ2dz8ne65L0nOSxkraJOkad/+6dW2iU40dOzZZHzduXG7tgQceKLkb1KOWkf8pSZMPeu4OSSvc/UxJK7LHAA4hheF397cl7Tjo6amS+rL7fZKmldwXgBZr9Dv/SHcfkKTs9pTyWgLQDi0/tt/MeiX1tno9AOrT6Mi/zcxGSVJ2uz3vhe4+39173L2nwXUBaIFGw79U0qzs/ixJS8ppB0C7FIbfzBZJ+rekX5hZv5ndKOl+SZPM7DNJk7LHAA4hhd/53X1mTuniknvBIai7uztZP+aYY3Jru3fvLrsd1IEj/ICgCD8QFOEHgiL8QFCEHwiK8ANBceluNGXKlCnJemqK7j179pTdDurAyA8ERfiBoAg/EBThB4Ii/EBQhB8IivADQbGfH01JnbIrSe6eW3v11VfLbgd1YOQHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaDYz38YOOGEE3JrkyZNSi47YcKEZH3y5IMnaP6hoim6U4qm6F6+fHmy/sYbbzS8bjDyA2ERfiAowg8ERfiBoAg/EBThB4Ii/EBQljrfWpLMbIGkKyRtd/dzsufulvQHSf/NXnanu/+zcGVm6ZUFNWPGjGT9qquuStYvvfTS3FrqGABJ2rt3b7K+a9euZL2rqytZf+2113Jrl112WXLZffv2Jevvvfdesv7II4/k1l544YXksocyd8+fLGGIWkb+pyQNd6TH39z93OyfwuAD6CyF4Xf3tyXtaEMvANqome/8t5rZh2a2wMxOKq0jAG3RaPgflXS6pHMlDUial/dCM+s1szVmtqbBdQFogYbC7+7b3P17d98v6TFJFyReO9/de9y9p9EmAZSvofCb2aghD6dLWldOOwDapfCUXjNbJOkiSSebWb+kuZIuMrNzJbmkTZJuamGPAFqgcD9/qSsLup9/4cKFyfoNN9yQrPf39yfrDz30UG7tnXfeSS67atWqZH3lypXJ+tq1a5P12267Lbd21FFHJZe9+uqrk/V77rknWR8zZkxubdGiRcllb7755mT9m2++SdarVOZ+fgCHIcIPBEX4gaAIPxAU4QeCIvxAUOzqK8H06dOT9cWLFyfrfX19yfrtt9+erO/cuTNZTymaYnvduvTxWxs2bEjWL7/88rp7KsvcuXMbqknF/15FlzzfsaO6c+HY1QcgifADQRF+ICjCDwRF+IGgCD8QFOEHgmI/f41S+8OLTotdsWJFsj5nzpyGeirDtGnTkvWnn346WZ8yZUqy/tZbb9XdUzsUTf89ceLEZP3ss89O1jdu3Fh3T2VhPz+AJMIPBEX4gaAIPxAU4QeCIvxAUIQfCKrwuv0Y1Nvbm1sr2uf74IMPlt1OaVKX/ZakzZs3J+uduh+/yKxZs5L1d999N1nfvXt3me1UgpEfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Iq3M9vZqMlLZR0qqT9kua7+9/NrEvSc5LGStok6Rp3/7p1rVZry5YtubX9+/e3sZP6XH/99cn66NGjk/WiOQkOVVu3bk3Wx40b16ZOqlPLyL9P0p/c/SxJv5Z0i5mdLekOSSvc/UxJK7LHAA4RheF39wF3/yC7v0vSekndkqZKOjDVTJ+k9CVhAHSUur7zm9lYSedJWiVppLsPSIN/ICSdUnZzAFqn5mP7zew4SS9KmuPuO81qukyYzKxXUv6B8QAqUdPIb2YjNBj8Z9z9pezpbWY2KquPkrR9uGXdfb6797h7TxkNAyhHYfhtcIh/QtJ6dx96etpSSQdOjZolaUn57QFolcJLd5vZBEkrJX2kwV19knSnBr/3Py9pjKQvJc1w9+S8xIfypbtTVq9enawvW7YsWb/rrruaWn/qsuJFp+QWXWL6wgsvbKgnVKfWS3cXfud393ck5b3ZxfU0BaBzcIQfEBThB4Ii/EBQhB8IivADQRF+ICgu3V2C++67L1l//PHHk/Ujj0z/Z1i7dm2yfuONN+bWTjzxxOSys2fPTtZx+GLkB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCs/nL3Vlh+n5/EVmzpyZrM+bNy9ZHzlyZLL+5ptv5tYefvjh5LJLlnANlsNNrefzM/IDQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFDs5wcOM+znB5BE+IGgCD8QFOEHgiL8QFCEHwiK8ANBFYbfzEab2Vtmtt7MPjazP2bP321mW8xsbfbP5a1vF0BZCg/yMbNRkka5+wdmdryk9yVNk3SNpN3u/teaV8ZBPkDL1XqQT+GMPe4+IGkgu7/LzNZL6m6uPQBVq+s7v5mNlXSepFXZU7ea2YdmtsDMTspZptfM1pjZmqY6BVCqmo/tN7PjJP1L0r3u/pKZjZT0lSSXdI8Gvxr8vuA92OwHWqzWzf6awm9mIyS9Iul1d39wmPpYSa+4+zkF70P4gRYr7cQeMzNJT0haPzT42Q+BB0yXtK7eJgFUp5Zf+ydIWinpI0n7s6fvlDRT0rka3OzfJOmm7MfB1Hsx8gMtVupmf1kIP9B6nM8PIInwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QVOEFPEv2laT/DHl8cvZcJ+rU3jq1L4neGlVmbz+r9YVtPZ//Rys3W+PuPZU1kNCpvXVqXxK9Naqq3tjsB4Ii/EBQVYd/fsXrT+nU3jq1L4neGlVJb5V+5wdQnapHfgAVqST8ZjbZzDaa2edmdkcVPeQxs01m9lE283ClU4xl06BtN7N1Q57rMrPlZvZZdjvsNGkV9dYRMzcnZpau9LPrtBmv277Zb2ZHSPpU0iRJ/ZJWS5rp7p+0tZEcZrZJUo+7V75P2Mx+I2m3pIUHZkMys79I2uHu92d/OE9y9z93SG93q86Zm1vUW97M0r9ThZ9dmTNel6GKkf8CSZ+7+xfuvkfSYklTK+ij47n725J2HPT0VEl92f0+Df7P03Y5vXUEdx9w9w+y+7skHZhZutLPLtFXJaoIf7ekzUMe96uzpvx2ScvM7H0z6626mWGMPDAzUnZ7SsX9HKxw5uZ2Omhm6Y757BqZ8bpsVYR/uNlEOmmXw3h3/5WkyyTdkm3eojaPSjpdg9O4DUiaV2Uz2czSL0qa4+47q+xlqGH6quRzqyL8/ZJGD3l8mqStFfQxLHffmt1ul/SyBr+mdJJtByZJzW63V9zP/7n7Nnf/3t33S3pMFX522czSL0p6xt1fyp6u/LMbrq+qPrcqwr9a0plmNs7MfirpWklLK+jjR8zs2OyHGJnZsZIuUefNPrxU0qzs/ixJSyrs5Qc6ZebmvJmlVfFn12kzXldykE+2K+MhSUdIWuDu97a9iWGY2c81ONpLg2c8Pltlb2a2SNJFGjzra5ukuZL+Iel5SWMkfSlphru3/Ye3nN4uUp0zN7eot7yZpVepws+uzBmvS+mHI/yAmDjCDwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUP8DFw4XZ1ldzXIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#파라미터 설정\n",
    "\n",
    "epochs = 100 #최대에폭\n",
    "learning_rate = 0.01\n",
    "batch_size = 2000\n",
    "dimension = 784\n",
    "max_data = train_x.shape[0]\n",
    "\n",
    "W = np.random.normal(loc=0.0, \n",
    "                        scale = np.sqrt(2/(dimension+10)), \n",
    "                        size = (dimension,10))\n",
    "b = 0\n",
    "\n",
    "train_batch = get_mini_batches(train_x, train_y, batch_size)[0] \n",
    "\n",
    "train_batch_x= train_batch[0]\n",
    "train_batch_y= train_batch[1]\n",
    "\n",
    "print(train_batch_x.shape)\n",
    "print(train_batch_y.shape)\n",
    "\n",
    "print(train_batch_y[42])\n",
    "plt.imshow(train_batch_x[42].reshape((28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = np.exp(X)\n",
    "    partition = X_exp.sum(axis=1, keepdims=True)\n",
    "    return X_exp / partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "[Train]  Loss : 0.051543    Acc :  69.0235\n",
      "[Dev]  Loss : 0.003160    Acc :  67.8571\n",
      "[Test]  Loss : 0.006131    Acc :  68.7857\n",
      "\n",
      "Epoch  2\n",
      "[Train]  Loss : 0.042092    Acc :  74.6975\n",
      "[Dev]  Loss : 0.002596    Acc :  73.4571\n",
      "[Test]  Loss : 0.005113    Acc :  73.9000\n",
      "\n",
      "Epoch  3\n",
      "[Train]  Loss : 0.038152    Acc :  77.0168\n",
      "[Dev]  Loss : 0.002315    Acc :  76.4286\n",
      "[Test]  Loss : 0.004586    Acc :  76.5143\n",
      "\n",
      "Epoch  4\n",
      "[Train]  Loss : 0.035922    Acc :  78.3630\n",
      "[Dev]  Loss : 0.002180    Acc :  77.7143\n",
      "[Test]  Loss : 0.004433    Acc :  77.1857\n",
      "\n",
      "Epoch  5\n",
      "[Train]  Loss : 0.034810    Acc :  79.0269\n",
      "[Dev]  Loss : 0.002132    Acc :  78.2571\n",
      "[Test]  Loss : 0.004264    Acc :  78.1429\n",
      "\n",
      "Epoch  6\n",
      "[Train]  Loss : 0.034199    Acc :  79.3933\n",
      "[Dev]  Loss : 0.002108    Acc :  78.4571\n",
      "[Test]  Loss : 0.004211    Acc :  78.4429\n",
      "\n",
      "Epoch  7\n",
      "[Train]  Loss : 0.033628    Acc :  79.7109\n",
      "[Dev]  Loss : 0.002024    Acc :  79.3143\n",
      "[Test]  Loss : 0.004122    Acc :  78.8857\n",
      "\n",
      "Epoch  8\n",
      "[Train]  Loss : 0.033408    Acc :  79.8605\n",
      "[Dev]  Loss : 0.002032    Acc :  79.1429\n",
      "[Test]  Loss : 0.004213    Acc :  78.3857\n",
      "\n",
      "Epoch  9\n",
      "[Train]  Loss : 0.033186    Acc :  79.9731\n",
      "[Dev]  Loss : 0.002027    Acc :  79.3143\n",
      "[Test]  Loss : 0.004047    Acc :  79.3000\n",
      "\n",
      "Epoch  10\n",
      "[Train]  Loss : 0.033014    Acc :  80.0992\n",
      "[Dev]  Loss : 0.001996    Acc :  79.5714\n",
      "[Test]  Loss : 0.004109    Acc :  78.8286\n",
      "\n",
      "Epoch  11\n",
      "[Train]  Loss : 0.034035    Acc :  79.4672\n",
      "[Dev]  Loss : 0.002073    Acc :  78.8286\n",
      "[Test]  Loss : 0.004332    Acc :  77.7714\n",
      "\n",
      "Epoch  12\n",
      "[Train]  Loss : 0.032651    Acc :  80.2958\n",
      "[Dev]  Loss : 0.001982    Acc :  79.7714\n",
      "[Test]  Loss : 0.004045    Acc :  79.1714\n",
      "\n",
      "Epoch  13\n",
      "[Train]  Loss : 0.033061    Acc :  80.0504\n",
      "[Dev]  Loss : 0.001973    Acc :  79.8571\n",
      "[Test]  Loss : 0.004029    Acc :  79.4000\n",
      "\n",
      "Epoch  14\n",
      "[Train]  Loss : 0.032621    Acc :  80.3193\n",
      "[Dev]  Loss : 0.002002    Acc :  79.6000\n",
      "[Test]  Loss : 0.004033    Acc :  79.3000\n",
      "\n",
      "Epoch  15\n",
      "[Train]  Loss : 0.032738    Acc :  80.2420\n",
      "[Dev]  Loss : 0.002017    Acc :  79.4286\n",
      "[Test]  Loss : 0.004078    Acc :  79.1000\n",
      "\n",
      "Epoch  16\n",
      "[Train]  Loss : 0.032535    Acc :  80.3681\n",
      "[Dev]  Loss : 0.001985    Acc :  79.7143\n",
      "[Test]  Loss : 0.004003    Acc :  79.5143\n",
      "\n",
      "Epoch  17\n",
      "[Train]  Loss : 0.032695    Acc :  80.2840\n",
      "[Dev]  Loss : 0.001995    Acc :  79.4857\n",
      "[Test]  Loss : 0.004068    Acc :  79.1714\n",
      "\n",
      "Epoch  18\n",
      "[Train]  Loss : 0.032358    Acc :  80.4756\n",
      "[Dev]  Loss : 0.002022    Acc :  79.2571\n",
      "[Test]  Loss : 0.004086    Acc :  78.9000\n",
      "\n",
      "Epoch  19\n",
      "[Train]  Loss : 0.032305    Acc :  80.5244\n",
      "[Dev]  Loss : 0.002015    Acc :  79.5429\n",
      "[Test]  Loss : 0.004033    Acc :  79.3857\n",
      "\n",
      "Epoch  20\n",
      "[Train]  Loss : 0.032362    Acc :  80.5076\n",
      "[Dev]  Loss : 0.002013    Acc :  79.4857\n",
      "[Test]  Loss : 0.004031    Acc :  79.4000\n",
      "\n",
      "Epoch  21\n",
      "[Train]  Loss : 0.032150    Acc :  80.6303\n",
      "[Dev]  Loss : 0.002007    Acc :  79.5714\n",
      "[Test]  Loss : 0.003993    Acc :  79.5000\n",
      "\n",
      "Epoch  22\n",
      "[Train]  Loss : 0.032488    Acc :  80.4134\n",
      "[Dev]  Loss : 0.002021    Acc :  79.4000\n",
      "[Test]  Loss : 0.004038    Acc :  79.2286\n",
      "\n",
      "Epoch  23\n",
      "[Train]  Loss : 0.032412    Acc :  80.4639\n",
      "[Dev]  Loss : 0.002016    Acc :  79.4000\n",
      "[Test]  Loss : 0.004032    Acc :  79.3143\n",
      "\n",
      "Epoch  24\n",
      "[Train]  Loss : 0.032547    Acc :  80.3916\n",
      "[Dev]  Loss : 0.002021    Acc :  79.4286\n",
      "[Test]  Loss : 0.004062    Acc :  79.2000\n",
      "\n",
      "Epoch  25\n",
      "[Train]  Loss : 0.032455    Acc :  80.4303\n",
      "[Dev]  Loss : 0.002017    Acc :  79.4000\n",
      "[Test]  Loss : 0.004038    Acc :  79.3857\n",
      "\n",
      "Epoch  26\n",
      "[Train]  Loss : 0.032508    Acc :  80.3866\n",
      "[Dev]  Loss : 0.002018    Acc :  79.3429\n",
      "[Test]  Loss : 0.004048    Acc :  79.3000\n",
      "\n",
      "Epoch  27\n",
      "[Train]  Loss : 0.032494    Acc :  80.3983\n",
      "[Dev]  Loss : 0.002019    Acc :  79.2857\n",
      "[Test]  Loss : 0.004041    Acc :  79.3429\n",
      "\n",
      "Epoch  28\n",
      "[Train]  Loss : 0.032492    Acc :  80.4151\n",
      "[Dev]  Loss : 0.002020    Acc :  79.3714\n",
      "[Test]  Loss : 0.004037    Acc :  79.3429\n",
      "\n",
      "Epoch  29\n",
      "[Train]  Loss : 0.032494    Acc :  80.4118\n",
      "[Dev]  Loss : 0.002019    Acc :  79.4286\n",
      "[Test]  Loss : 0.004031    Acc :  79.3714\n",
      "\n",
      "Epoch  30\n",
      "[Train]  Loss : 0.032498    Acc :  80.4067\n",
      "[Dev]  Loss : 0.002018    Acc :  79.4286\n",
      "[Test]  Loss : 0.004027    Acc :  79.4000\n",
      "\n",
      "Epoch  31\n",
      "[Train]  Loss : 0.032502    Acc :  80.4084\n",
      "[Dev]  Loss : 0.002017    Acc :  79.4000\n",
      "[Test]  Loss : 0.004024    Acc :  79.4714\n",
      "\n",
      "Epoch  32\n",
      "[Train]  Loss : 0.032507    Acc :  80.4134\n",
      "[Dev]  Loss : 0.002016    Acc :  79.4571\n",
      "[Test]  Loss : 0.004023    Acc :  79.4429\n",
      "\n",
      "Epoch  33\n",
      "[Train]  Loss : 0.032498    Acc :  80.4067\n",
      "[Dev]  Loss : 0.002015    Acc :  79.4857\n",
      "[Test]  Loss : 0.004022    Acc :  79.4000\n",
      "\n",
      "Epoch  34\n",
      "[Train]  Loss : 0.032482    Acc :  80.4151\n",
      "[Dev]  Loss : 0.002014    Acc :  79.4857\n",
      "[Test]  Loss : 0.004020    Acc :  79.4286\n",
      "\n",
      "Epoch  35\n",
      "[Train]  Loss : 0.032467    Acc :  80.4252\n",
      "[Dev]  Loss : 0.002013    Acc :  79.4571\n",
      "[Test]  Loss : 0.004019    Acc :  79.4571\n",
      "\n",
      "Epoch  36\n",
      "[Train]  Loss : 0.032452    Acc :  80.4403\n",
      "[Dev]  Loss : 0.002012    Acc :  79.4286\n",
      "[Test]  Loss : 0.004017    Acc :  79.4714\n",
      "\n",
      "Epoch  37\n",
      "[Train]  Loss : 0.032438    Acc :  80.4504\n",
      "[Dev]  Loss : 0.002012    Acc :  79.4000\n",
      "[Test]  Loss : 0.004016    Acc :  79.4714\n",
      "\n",
      "Epoch  38\n",
      "[Train]  Loss : 0.032426    Acc :  80.4571\n",
      "[Dev]  Loss : 0.002011    Acc :  79.3714\n",
      "[Test]  Loss : 0.004015    Acc :  79.4857\n",
      "\n",
      "Epoch  39\n",
      "[Train]  Loss : 0.032413    Acc :  80.4639\n",
      "[Dev]  Loss : 0.002011    Acc :  79.4000\n",
      "[Test]  Loss : 0.004014    Acc :  79.4857\n",
      "\n",
      "Epoch  40\n",
      "[Train]  Loss : 0.032402    Acc :  80.4723\n",
      "[Dev]  Loss : 0.002010    Acc :  79.4000\n",
      "[Test]  Loss : 0.004012    Acc :  79.4857\n",
      "\n",
      "Epoch  41\n",
      "[Train]  Loss : 0.032391    Acc :  80.4739\n",
      "[Dev]  Loss : 0.002010    Acc :  79.4000\n",
      "[Test]  Loss : 0.004011    Acc :  79.4857\n",
      "\n",
      "Early Stopping !\n"
     ]
    }
   ],
   "source": [
    "# train, test\n",
    "\n",
    "dev_temp = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    for j in range(int(max_data/batch_size)):   \n",
    "        \n",
    "        gradient_w =  np.dot(np.transpose(train_batch_x), (softmax(np.dot(train_batch_x,W) + b) - train_batch_y ))/d_size\n",
    "        gradient_b =  np.sum((np.dot(train_batch_x,W) + b - train_batch_y )* 2) /d_size\n",
    "\n",
    "        W -= learning_rate * gradient_w\n",
    "        b -= learning_rate * gradient_b\n",
    "\n",
    "    train_hypothesis = softmax(np.dot(train_x,W) + b)\n",
    "    train_est = np.argmax(train_hypothesis, axis=1) - np.argmax(train_y, axis=1)\n",
    "    train_Loss = np.sum((train_hypothesis - train_y) ** 2) / d_size*0.1\n",
    "    train_acc= len(np.where(train_est==0)[0])/int(d_size*0.85)*100\n",
    "    \n",
    "    dev_hypothesis = softmax(np.dot(dev_x,W) + b)\n",
    "    dev_est = np.argmax(dev_hypothesis, axis=1) - np.argmax(dev_y, axis=1)\n",
    "    dev_Loss = np.sum((dev_hypothesis - dev_y) ** 2) / d_size*0.1\n",
    "    dev_acc = len(np.where(dev_est==0)[0])/int(d_size*0.05)*100\n",
    "    \n",
    "    test_hypothesis = softmax(np.dot(test_x,W) + b)\n",
    "    test_est = np.argmax(test_hypothesis, axis=1) - np.argmax(test_y, axis=1)\n",
    "    test_Loss = np.sum((test_hypothesis - test_y) ** 2) / d_size*0.1\n",
    "    test_acc = len(np.where(test_est==0)[0])/int(d_size*0.1)*100\n",
    "    \n",
    "    \n",
    "    print('Epoch  {:d}'.format(i+1))\n",
    "    print('[Train]  Loss : {:f}    Acc :  {:.4f}'.format(np.sum(train_Loss), train_acc))\n",
    "    print('[Dev]  Loss : {:f}    Acc :  {:.4f}'.format(np.sum(dev_Loss), dev_acc))\n",
    "    print('[Test]  Loss : {:f}    Acc :  {:.4f}'.format(np.sum(test_Loss), test_acc))\n",
    "    print()\n",
    "\n",
    "    if(abs(dev_acc-dev_temp)<0.0001):\n",
    "        print(\"Early Stopping !\")\n",
    "        break\n",
    "    \n",
    "    if((i+1)%5==0):\n",
    "        dev_temp = dev_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Multi-layer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from collections import OrderedDict\n",
    "\n",
    "X, y = datasets.fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =np.expand_dims(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(y)\n",
    "y= enc.transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 섞어주기 위해 셔플\n",
    "\n",
    "tmp = [[a,b] for a, b in zip(X, y)]\n",
    "import random\n",
    "\n",
    "random.shuffle(tmp)\n",
    "\n",
    "X = [n[0] for n in tmp]\n",
    "y = [n[1] for n in tmp]\n",
    "\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_size = 70000 \n",
    "\n",
    "\n",
    "# 학습용데이터\n",
    "train_x = X[0:int(d_size*0.85)]\n",
    "train_y = y[0:int(d_size*0.85)]\n",
    "\n",
    "# dev 데이터\n",
    "dev_x = X[int(d_size*0.85):int(d_size*0.9)]\n",
    "dev_y = y[int(d_size*0.85):int(d_size*0.9)]\n",
    "\n",
    "# test 데이터\n",
    "test_x = X[int(d_size*0.9):d_size]\n",
    "test_y = y[int(d_size*0.9):d_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x = x.astype(np.float32)\n",
    "    x /= 255.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = normalize(train_x)\n",
    "dev_x = normalize(dev_x)\n",
    "test_x = normalize(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59500, 784)\n",
      "(59500, 10)\n",
      "(3500, 784)\n",
      "(3500, 10)\n",
      "(7000, 784)\n",
      "(7000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(dev_x.shape)\n",
    "print(dev_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 가중치와 편향 매개변수의 미분\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 텐서 대응\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t): # negative log likelihood\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실함수\n",
    "        self.y = None    # softmax의 출력\n",
    "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layers_size, weight_init_std = 0.01):\n",
    "        \n",
    "        temp = len(layers_size)-2 # 히든 레이어 갯수\n",
    "        self.temp = temp\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        \n",
    "        self.params['W1'] = weight_init_std * np.random.randn(layers_size[0], layers_size[1])\n",
    "        self.params['b1'] = np.zeros(layers_size[1])\n",
    "        \n",
    "        for i in range(temp):\n",
    "            self.params['W'+str(i+2)] = weight_init_std * np.random.randn(layers_size[i+1], layers_size[i+2])\n",
    "            self.params['b'+str(i+2)] = np.zeros(layers_size[i+2])\n",
    "            \n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        for i in range(temp):\n",
    "            self.layers['Relu'+str(i+1)] = Relu()\n",
    "            self.layers['Affine'+str(i+2)] = Affine(self.params['W'+str(i+2)], self.params['b'+str(i+2)])\n",
    "           \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers: #역전파\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        \n",
    "        for i in range(self.temp):\n",
    "            grads['W'+str(i+2)], grads['b'+str(i+2)] = self.layers['Affine'+str(i+2)].dW, self.layers['Affine'+str(i+2)].db\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정\n",
    "\n",
    "#network = MLP([784,50,10]) # input(784고정) - [hidden] - output(10고정: 10개로 분류)\n",
    "#network = MLP([784,50,50,10])\n",
    "network = MLP([784,50,50,50,10])\n",
    "#network = MLP([784,50,50,50,50,10])\n",
    "\n",
    "iters_num = 10000000\n",
    "train_size = train_x.shape[0]\n",
    "batch_size = 500\n",
    "learning_rate = 0.5\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "max_epoch = 100 # default = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "[Train]  Loss : 2.301284    Acc :  11.1916\n",
      "[Dev]  Loss : 2.299928    Acc :  11.8286\n",
      "[Test]  Loss : 2.301620    Acc :  11.4857\n",
      "\n",
      "Epoch  2\n",
      "[Train]  Loss : 2.273938    Acc :  23.6437\n",
      "[Dev]  Loss : 2.272735    Acc :  24.2571\n",
      "[Test]  Loss : 2.275435    Acc :  23.2286\n",
      "\n",
      "Epoch  3\n",
      "[Train]  Loss : 1.832531    Acc :  29.0655\n",
      "[Dev]  Loss : 1.827142    Acc :  29.6571\n",
      "[Test]  Loss : 1.834598    Acc :  28.2286\n",
      "\n",
      "Epoch  4\n",
      "[Train]  Loss : 0.968142    Acc :  62.4639\n",
      "[Dev]  Loss : 0.964119    Acc :  62.8571\n",
      "[Test]  Loss : 0.970620    Acc :  62.3571\n",
      "\n",
      "Epoch  5\n",
      "[Train]  Loss : 0.356292    Acc :  90.7395\n",
      "[Dev]  Loss : 0.355561    Acc :  90.8000\n",
      "[Test]  Loss : 0.352757    Acc :  90.8714\n",
      "\n",
      "Epoch  6\n",
      "[Train]  Loss : 0.247245    Acc :  93.2538\n",
      "[Dev]  Loss : 0.259978    Acc :  93.2857\n",
      "[Test]  Loss : 0.255517    Acc :  92.9571\n",
      "\n",
      "Epoch  7\n",
      "[Train]  Loss : 0.197712    Acc :  94.6672\n",
      "[Dev]  Loss : 0.214880    Acc :  94.5714\n",
      "[Test]  Loss : 0.205054    Acc :  94.5143\n",
      "\n",
      "Epoch  8\n",
      "[Train]  Loss : 0.167026    Acc :  95.2151\n",
      "[Dev]  Loss : 0.186493    Acc :  94.9714\n",
      "[Test]  Loss : 0.177573    Acc :  94.9286\n",
      "\n",
      "Epoch  9\n",
      "[Train]  Loss : 0.152314    Acc :  95.5815\n",
      "[Dev]  Loss : 0.182840    Acc :  94.9143\n",
      "[Test]  Loss : 0.172935    Acc :  95.2857\n",
      "\n",
      "Early Stopping !\n"
     ]
    }
   ],
   "source": [
    "wb_arrange = ['W1', 'b1']\n",
    "tem=1\n",
    "dev_temp=0\n",
    "for i in range(network.temp):\n",
    "    wb_arrange.append('W'+str(i+2))\n",
    "    wb_arrange.append('b'+str(i+2))\n",
    "    \n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = train_x[batch_mask]\n",
    "    t_batch = train_y[batch_mask]\n",
    "    \n",
    "    # backpropa\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # update\n",
    "    for key in wb_arrange:\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(train_x, train_y)\n",
    "        dev_acc = network.accuracy(dev_x, dev_y)\n",
    "        test_acc = network.accuracy(test_x, test_y)\n",
    "        \n",
    "        train_loss = network.loss(train_x, train_y)\n",
    "        dev_loss = network.loss(dev_x, dev_y)\n",
    "        test_loss = network.loss(test_x, test_y)\n",
    "        print('Epoch  {:d}'.format(tem))\n",
    "        print('[Train]  Loss : {:f}    Acc :  {:.4f}'.format(train_loss, train_acc*100))\n",
    "        print('[Dev]  Loss : {:f}    Acc :  {:.4f}'.format(dev_loss, dev_acc*100))\n",
    "        print('[Test]  Loss : {:f}    Acc :  {:.4f}'.format(test_loss, test_acc*100))\n",
    "        print()\n",
    "        \n",
    "        if tem==max_epoch:\n",
    "            break        \n",
    "        tem=tem+1\n",
    "        \n",
    "        \n",
    "        if(abs(dev_acc*100-dev_temp)<0.2):\n",
    "            print(\"Early Stopping !\")\n",
    "            break\n",
    "\n",
    "        dev_temp = dev_acc*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle 이용해 저장\n",
    "import pickle\n",
    "\n",
    "for i in wb_arrange:\n",
    "    with open(\"MLP/MLP_mnist_\"+i+\".pickle\",\"wb\") as fw:\n",
    "        pickle.dump(network.params[i], fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러온 파라미터를 집어넣을 새 모델 생성(layers 의 갯수와 size를 맞게 설정해야된다.)\n",
    "network2 = MLP([784,50,50,50,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle 이용해 불러오기\n",
    "\n",
    "for i in range(network2.temp+1):\n",
    "    with open(\"MLP/MLP_mnist_\"+'W'+str(i+1)+\".pickle\",\"rb\") as fw:\n",
    "        with open(\"MLP/MLP_mnist_\"+'b'+str(i+1)+\".pickle\",\"rb\") as fb:\n",
    "            network2.layers['Affine'+str(i+1)] = Affine(pickle.load(fw), pickle.load(fb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.172935    Acc :  95.2857\n"
     ]
    }
   ],
   "source": [
    "# 불러온 학습된 모델을 통해 테스트(테스트셋에서)\n",
    "\n",
    "acc = network2.accuracy(test_x, test_y)\n",
    "loss = network2.loss(test_x, test_y)\n",
    "\n",
    "print('Loss : {:f}    Acc :  {:.4f}'.format(loss, acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
