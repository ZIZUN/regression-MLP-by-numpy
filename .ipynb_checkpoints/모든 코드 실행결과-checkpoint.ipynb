{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Numpy: Tutorial\n",
    "## 1.1 dot, einsum\n",
    "### dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7 10]\n",
      " [15 22]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2],[3,4]])\n",
    "b = a\n",
    "\n",
    "print(np.dot(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "a=4\n",
    "\n",
    "print(np.dot(a,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(24).reshape((2,3,4))\n",
    "b = np.arange(24).reshape((2,4,3))\n",
    "print(np.dot(a,b).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(6).reshape((3,2,1))\n",
    "b = np.arange(6).reshape((3,1,2))\n",
    "\n",
    "print(np.dot(a,b).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(16).reshape((2,2,4))\n",
    "b = np.arange(16).reshape((2,4,2))\n",
    "print(np.dot(a,b).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n"
     ]
    }
   ],
   "source": [
    "# Transpose\n",
    "\n",
    "A = np.array([[1,2,3], [4,5,6]])\n",
    "R = np.einsum(\"ij->ji\", A)\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "# diagonal, Trace\n",
    "\n",
    "A = np.eye(10)\n",
    "print(A)\n",
    "diag = np.einsum('ii->i', A)\n",
    "trace =np.einsum('ii->', A)\n",
    "print(diag)\n",
    "print(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "# matrix sum to scalar\n",
    "\n",
    "A = np.array([[1,2,3], [4,5,6]])\n",
    "R = np.einsum(\"ij->\", A)\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6 15]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "# matrix column or row sum (to vector)\n",
    "\n",
    "A = np.array([[1,2,3], [4,5,6]])\n",
    "row_sum = np.einsum(\"ij->i\", A)\n",
    "col_sum =np.einsum(\"ij->j\", A)\n",
    "print(row_sum)\n",
    "print(col_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10101\n",
      "[[    -1    -10   -100]\n",
      " [   -10   -100  -1000]\n",
      " [  -100  -1000 -10000]]\n"
     ]
    }
   ],
   "source": [
    "# Dot Product, Outer product of two vectors\n",
    "\n",
    "x = np.array([-1, -10, -100])\n",
    "y = np.array([1, 10, 100])\n",
    "dot = np.einsum('i,i->', x, y ) # dot product\n",
    "outer = np.einsum('i,j->ij', x,y) # outer product\n",
    "print(dot)\n",
    "print(outer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    -1   -100 -10000]\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "# Hadamard(element-wise) product of vector or matrix\n",
    "\n",
    "x = np.array([-1, -10, -100])\n",
    "y = np.array([1, 10, 100])\n",
    "elemwise_vec = np.einsum('i,i->i', x, y)\n",
    "print(elemwise_vec)\n",
    "A = np.arange(6).reshape((2, 3))\n",
    "B = np.arange(6).reshape((2, 3))\n",
    "elemwise_mat = np.einsum('ij,ij->', A, B)\n",
    "print(elemwise_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-321 -654]\n",
      "[[14 32]\n",
      " [32 77]]\n",
      "[[[0.40298129 0.59043898]\n",
      "  [0.0739351  0.10832802]]\n",
      "\n",
      " [[0.24378779 0.07427415]\n",
      "  [0.4942785  0.15059046]]\n",
      "\n",
      " [[0.06461978 0.14414622]\n",
      "  [0.24197069 0.53975978]]]\n",
      "(3, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "# Matrix-Vector multiplication\n",
    "\n",
    "A = np.array([[1,2,3], [4,5,6]])\n",
    "x = np.array([-1, -10, -100])\n",
    "b = np.einsum('ij,j->i', A, x)\n",
    "print(b)\n",
    "\n",
    "## Matrix-Matrix Multiplication\n",
    "A = np.array([[1,2,3], [4,5,6]])\n",
    "b = A.transpose()\n",
    "R = np.einsum('ik,kj->ij', A, b)\n",
    "print(R)\n",
    "\n",
    "## Batched Matrix Multiplication\n",
    "A = np.random.random(size=(3,2,1))\n",
    "B = np.random.random(size=(3,1,2))\n",
    "\n",
    "R = np.einsum('bik,bkj->bij',A, B)\n",
    "print(R)\n",
    "print(R.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-18.550454732519377\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2,3])\n",
    "y = np.array([-1,-2,-3])\n",
    "A = np.random.random(size=(3, 3))\n",
    "\n",
    "r = np.einsum('i,ij,j->', x, A, y)\n",
    "\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 quiz풀이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  4]\n",
      " [ 5  6  7  8  9]\n",
      " [10 11 12 13 14]\n",
      " [15 16 17 18 19]\n",
      " [20 21 22 23 24]]\n",
      "[[ 5  6  7  8  9]\n",
      " [ 0  1  2  3  4]\n",
      " [10 11 12 13 14]\n",
      " [15 16 17 18 19]\n",
      " [20 21 22 23 24]]\n"
     ]
    }
   ],
   "source": [
    "# 72. How to swap two rows of an array? (★★★)\n",
    "\n",
    "A = np.arange(25).reshape(5,5)\n",
    "print(A)\n",
    "A[[0,1]] = A[[1,0]]\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 15,  54, 111])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 69. How to get the diagonal of a dot product? (★★★)\n",
    "\n",
    "A = np.arange(9).reshape((3,3))\n",
    "B = np.arange(9).reshape((3,3))\n",
    "\n",
    "np.einsum(\"ij,ji->i\", A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2 3 4 4 6]\n"
     ]
    }
   ],
   "source": [
    "# 74. Given an array C that is a bincount, how to produce an array A such that np.bincount(A) == C? (★★★)\n",
    "\n",
    "C = np.bincount([1,1,2,3,4,4,6])\n",
    "A = np.repeat(np.arange(len(C)), C)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 2 4 5 8 8 7 0 0 6 6 6 5 1 9 4 0 1 6 1 8 8 6 1 9 9 7 9 2 9 4 9 4 7 6 1 5\n",
      " 4 3 5 3 7 9 0 0 4 7 0 3 9]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# 83. How to find the most frequent value in an array? (★★★)\n",
    "Z = np.random.randint(0,10,50)\n",
    "print(Z)\n",
    "print(np.bincount(Z).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9989 9990 9991 9992 9993 9994 9995 9996 9997 9998 9999]\n"
     ]
    }
   ],
   "source": [
    "# 89. How to get the n largest values of an array (★★★)\n",
    "\n",
    "Z = np.arange(10000)\n",
    "np.random.shuffle(Z)\n",
    "n = 11\n",
    "\n",
    "print (Z[np.argsort(Z)[-n:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 선형방정식 풀기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.5  0.5 -0.5]\n",
      " [ 0.   1.   2. ]]\n",
      "[[ 3.60000000e-01  1.60000000e-01 -4.00000000e-02]\n",
      " [ 2.40000000e-01  1.40000000e-01  4.00000000e-02]\n",
      " [ 1.20000000e-01  1.20000000e-01  1.20000000e-01]\n",
      " [-4.16333634e-17  1.00000000e-01  2.00000000e-01]\n",
      " [-1.20000000e-01  8.00000000e-02  2.80000000e-01]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import inv\n",
    "# Ax = b 풀기\n",
    "\n",
    "A = np.arange(4).reshape((2,2))\n",
    "b = np.arange(6).reshape((2,3))\n",
    "\n",
    "print(np.dot(np.linalg.inv(A),b))\n",
    "\n",
    "# pseudo inverse 이용 (SVD)\n",
    "\n",
    "A = np.arange(10).reshape((2,5))\n",
    "b = np.arange(6).reshape((2,3))\n",
    "\n",
    "print(np.dot(np.linalg.pinv(A),b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 만들고 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100)\n",
      "(1,)\n",
      "(10000, 100)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "\n",
    "## 데이터셋 만들고 저장\n",
    "\n",
    "w = 20*np.random.rand(1,100)-10 \n",
    "b = 20*np.random.rand(1)-10\n",
    "\n",
    "x = 20*np.random.rand(10000,100,1)-10\n",
    "y = np.matmul(w,x)\n",
    "\n",
    "x=np.squeeze(x, axis=2)\n",
    "y=np.squeeze(y, axis=2)+b\n",
    "\n",
    "print(w.shape)\n",
    "print(b.shape)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "\n",
    "# pickle 이용해 저장\n",
    "\n",
    "with open(\"linear/myrandomdataset_w.pickle\",\"wb\") as fw:\n",
    "    pickle.dump(w, fw)\n",
    "with open(\"linear/myrandomdataset_b.pickle\",\"wb\") as fw:\n",
    "    pickle.dump(b, fw)\n",
    "with open(\"linear/myrandomdataset_x.pickle\",\"wb\") as fw:\n",
    "    pickle.dump(x, fw)\n",
    "with open(\"linear/myrandomdataset_y.pickle\",\"wb\") as fw:\n",
    "    pickle.dump(y, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(y.shape)\n",
    "plt.figure\n",
    "plt.hist(y, 444)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "\n",
    "#배치 뽑는 함수\n",
    "\n",
    "def get_mini_batches(X, y, batch_size): \n",
    "    random_idxs = np.random.choice(len(y), len(y), replace=False)\n",
    "    X_shuffled = X[random_idxs,:]\n",
    "    y_shuffled = y[random_idxs]\n",
    "    mini_batches = [(X_shuffled[i:i+batch_size,:], y_shuffled[i:i+batch_size]) for\n",
    "                   i in range(0, len(y), batch_size)]\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle 이용해 읽기\n",
    "\n",
    "with open(\"linear/myrandomdataset_w.pickle\",\"rb\") as fr:\n",
    "    w_ = pickle.load(fr)\n",
    "with open(\"linear/myrandomdataset_b.pickle\",\"rb\") as fr:\n",
    "    b_ = pickle.load(fr)\n",
    "with open(\"linear/myrandomdataset_x.pickle\",\"rb\") as fr:\n",
    "    x_ = pickle.load(fr)\n",
    "with open(\"linear/myrandomdataset_y.pickle\",\"rb\") as fr:\n",
    "    y_ = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442, 1)\n"
     ]
    }
   ],
   "source": [
    "aa_=diabetes['data']\n",
    "bb_ =np.expand_dims(diabetes['target'], axis=1)\n",
    "\n",
    "print(aa_.shape)\n",
    "print(bb_.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 100)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "d_size = 1000 #deafult   N=1000\n",
    "if(x_.shape[0]==10000): # N의 사이즈에 따른 설정 (1000, 10000, 100000 중 택 1)\n",
    "    d_size=10000\n",
    "elif(x_.shape[0]==100000):\n",
    "    d_size=100000\n",
    "elif(x_.shape[0]==442):\n",
    "    d_size=442\n",
    "    \n",
    "# 학습용데이터\n",
    "train_x = x_[0:int(d_size*0.85)]\n",
    "train_y = y_[0:int(d_size*0.85)]\n",
    "\n",
    "# dev 데이터\n",
    "dev_x = x_[int(d_size*0.85):int(d_size*0.9)]\n",
    "dev_y = y_[int(d_size*0.85):int(d_size*0.9)]\n",
    "\n",
    "# test 데이터\n",
    "test_x = x_[int(d_size*0.9):d_size]\n",
    "test_y = y_[int(d_size*0.9):d_size]\n",
    "\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 100)\n",
      "(300, 1)\n"
     ]
    }
   ],
   "source": [
    "#파라미터 설정\n",
    "\n",
    "epochs = 100 # 최대 에폭\n",
    "learning_rate = 0.01\n",
    "batch_size = 300\n",
    "dimension = 100\n",
    "max_data = train_x.shape[0]\n",
    "\n",
    "W = np.zeros((dimension,1))\n",
    "b = 0\n",
    "\n",
    "train_batch = get_mini_batches(train_x, train_y, batch_size)[0] \n",
    "\n",
    "train_batch_x= train_batch[0]\n",
    "train_batch_y= train_batch[1]\n",
    "\n",
    "print(train_batch_x.shape)\n",
    "print(train_batch_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "[Train Loss]  0.889868\n",
      "[Dev Loss]  0.053760\n",
      "[Test Loss]  0.106768\n",
      "w squared error : 0.001839 , b squared error : 0.021464\n",
      "\n",
      "Epoch  2\n",
      "[Train Loss]  0.309045\n",
      "[Dev Loss]  0.018521\n",
      "[Test Loss]  0.036291\n",
      "w squared error : 0.000423 , b squared error : 0.015069\n",
      "\n",
      "Epoch  3\n",
      "[Train Loss]  0.149658\n",
      "[Dev Loss]  0.008971\n",
      "[Test Loss]  0.017576\n",
      "w squared error : 0.000203 , b squared error : 0.010520\n",
      "\n",
      "Epoch  4\n",
      "[Train Loss]  0.072908\n",
      "[Dev Loss]  0.004370\n",
      "[Test Loss]  0.008563\n",
      "w squared error : 0.000099 , b squared error : 0.007344\n",
      "\n",
      "Epoch  5\n",
      "[Train Loss]  0.035525\n",
      "[Dev Loss]  0.002129\n",
      "[Test Loss]  0.004172\n",
      "w squared error : 0.000048 , b squared error : 0.005126\n",
      "\n",
      "Epoch  6\n",
      "[Train Loss]  0.017310\n",
      "[Dev Loss]  0.001038\n",
      "[Test Loss]  0.002033\n",
      "w squared error : 0.000023 , b squared error : 0.003578\n",
      "\n",
      "Epoch  7\n",
      "[Train Loss]  0.008434\n",
      "[Dev Loss]  0.000506\n",
      "[Test Loss]  0.000991\n",
      "w squared error : 0.000011 , b squared error : 0.002498\n",
      "\n",
      "Epoch  8\n",
      "[Train Loss]  0.004110\n",
      "[Dev Loss]  0.000246\n",
      "[Test Loss]  0.000483\n",
      "w squared error : 0.000006 , b squared error : 0.001744\n",
      "\n",
      "Epoch  9\n",
      "[Train Loss]  0.002003\n",
      "[Dev Loss]  0.000120\n",
      "[Test Loss]  0.000235\n",
      "w squared error : 0.000003 , b squared error : 0.001217\n",
      "\n",
      "Epoch  10\n",
      "[Train Loss]  0.000976\n",
      "[Dev Loss]  0.000058\n",
      "[Test Loss]  0.000115\n",
      "w squared error : 0.000001 , b squared error : 0.000850\n",
      "\n",
      "Epoch  11\n",
      "[Train Loss]  0.000475\n",
      "[Dev Loss]  0.000028\n",
      "[Test Loss]  0.000056\n",
      "w squared error : 0.000001 , b squared error : 0.000593\n",
      "\n",
      "Epoch  12\n",
      "[Train Loss]  0.000232\n",
      "[Dev Loss]  0.000014\n",
      "[Test Loss]  0.000027\n",
      "w squared error : 0.000000 , b squared error : 0.000414\n",
      "\n",
      "Epoch  13\n",
      "[Train Loss]  0.000113\n",
      "[Dev Loss]  0.000007\n",
      "[Test Loss]  0.000013\n",
      "w squared error : 0.000000 , b squared error : 0.000289\n",
      "\n",
      "Epoch  14\n",
      "[Train Loss]  0.000055\n",
      "[Dev Loss]  0.000003\n",
      "[Test Loss]  0.000006\n",
      "w squared error : 0.000000 , b squared error : 0.000202\n",
      "\n",
      "Epoch  15\n",
      "[Train Loss]  0.000027\n",
      "[Dev Loss]  0.000002\n",
      "[Test Loss]  0.000003\n",
      "w squared error : 0.000000 , b squared error : 0.000141\n",
      "\n",
      "Epoch  16\n",
      "[Train Loss]  0.000013\n",
      "[Dev Loss]  0.000001\n",
      "[Test Loss]  0.000002\n",
      "w squared error : 0.000000 , b squared error : 0.000098\n",
      "\n",
      "Epoch  17\n",
      "[Train Loss]  0.000006\n",
      "[Dev Loss]  0.000000\n",
      "[Test Loss]  0.000001\n",
      "w squared error : 0.000000 , b squared error : 0.000069\n",
      "\n",
      "Epoch  18\n",
      "[Train Loss]  0.000003\n",
      "[Dev Loss]  0.000000\n",
      "[Test Loss]  0.000000\n",
      "w squared error : 0.000000 , b squared error : 0.000048\n",
      "\n",
      "Epoch  19\n",
      "[Train Loss]  0.000002\n",
      "[Dev Loss]  0.000000\n",
      "[Test Loss]  0.000000\n",
      "w squared error : 0.000000 , b squared error : 0.000033\n",
      "\n",
      "Epoch  20\n",
      "[Train Loss]  0.000001\n",
      "[Dev Loss]  0.000000\n",
      "[Test Loss]  0.000000\n",
      "w squared error : 0.000000 , b squared error : 0.000023\n",
      "\n",
      "Epoch  21\n",
      "[Train Loss]  0.000000\n",
      "[Dev Loss]  0.000000\n",
      "[Test Loss]  0.000000\n",
      "w squared error : 0.000000 , b squared error : 0.000016\n",
      "\n",
      "Epoch  22\n",
      "[Train Loss]  0.000000\n",
      "[Dev Loss]  0.000000\n",
      "[Test Loss]  0.000000\n",
      "w squared error : 0.000000 , b squared error : 0.000011\n",
      "\n",
      "Epoch  23\n",
      "[Train Loss]  0.000000\n",
      "[Dev Loss]  0.000000\n",
      "[Test Loss]  0.000000\n",
      "w squared error : 0.000000 , b squared error : 0.000008\n",
      "\n",
      "Epoch  24\n",
      "[Train Loss]  0.000000\n",
      "[Dev Loss]  0.000000\n",
      "[Test Loss]  0.000000\n",
      "w squared error : 0.000000 , b squared error : 0.000006\n",
      "\n",
      "Early Stopping !\n"
     ]
    }
   ],
   "source": [
    "# train, test\n",
    "\n",
    "dev_temp = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    for j in range(int(max_data/batch_size)):    \n",
    "        gradient_w =  np.dot(np.transpose(train_batch_x), (np.dot(train_batch_x,W) + b - train_batch_y ))* 2 / batch_size\n",
    "        gradient_b =  np.sum((np.dot(train_batch_x,W) + b - train_batch_y )* 2) / batch_size\n",
    "\n",
    "        W -= learning_rate * gradient_w\n",
    "        b -= learning_rate * gradient_b\n",
    "    \n",
    "        hypothesis = np.dot(train_batch_x,W) + b\n",
    "        Loss = np.sum((hypothesis - train_batch_y) ** 2) / batch_size\n",
    "\n",
    "    train_hypothesis = np.dot(train_x,W) + b\n",
    "    train_Loss = np.sum((train_hypothesis - train_y) ** 2) / d_size*0.1\n",
    "        \n",
    "    dev_hypothesis = np.dot(dev_x,W) + b\n",
    "    dev_Loss = np.sum((dev_hypothesis - dev_y) ** 2) / d_size*0.1\n",
    "    \n",
    "    test_hypothesis = np.dot(test_x,W) + b\n",
    "    test_Loss = np.sum((test_hypothesis - test_y) ** 2) / d_size*0.1\n",
    "    \n",
    "    w_squared_error = np.sum((W-np.transpose(w_))** 2) / dimension\n",
    "    b_squared_error = np.sum(abs(b-b_)) / dimension\n",
    "\n",
    "    \n",
    "    print('Epoch  {:d}\\n[Train Loss]  {:f}'.format(i+1, train_Loss))\n",
    "    print('[Dev Loss]  {:f}'.format(np.sum(dev_Loss)))\n",
    "    print('[Test Loss]  {:f}'.format(np.sum(test_Loss)))\n",
    "    print('w squared error : {:f} , b squared error : {:f}'.format(w_squared_error, b_squared_error))\n",
    "    print()\n",
    "\n",
    "    if(dev_temp != 0 and (dev_temp - dev_Loss<0 or dev_temp - dev_Loss<0.00000001)):\n",
    "        print(\"Early Stopping !\")\n",
    "        break\n",
    "\n",
    "    dev_temp = test_Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "X, y = datasets.fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =np.expand_dims(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(y)\n",
    "y= enc.transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 섞어주기 위해 셔플\n",
    "\n",
    "tmp = [[a,b] for a, b in zip(X, y)]\n",
    "import random\n",
    "\n",
    "random.shuffle(tmp)\n",
    "\n",
    "X = [n[0] for n in tmp]\n",
    "y = [n[1] for n in tmp]\n",
    "\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_size = 70000 \n",
    "\n",
    "\n",
    "# 학습용데이터\n",
    "train_x = X[0:int(d_size*0.85)]\n",
    "train_y = y[0:int(d_size*0.85)]\n",
    "\n",
    "# dev 데이터\n",
    "dev_x = X[int(d_size*0.85):int(d_size*0.9)]\n",
    "dev_y = y[int(d_size*0.85):int(d_size*0.9)]\n",
    "\n",
    "# test 데이터\n",
    "test_x = X[int(d_size*0.9):d_size]\n",
    "test_y = y[int(d_size*0.9):d_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 784)\n",
      "(2000, 10)\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25e27169f28>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#파라미터 설정\n",
    "\n",
    "epochs = 100 #최대에폭\n",
    "learning_rate = 0.01\n",
    "batch_size = 2000\n",
    "dimension = 784\n",
    "max_data = train_x.shape[0]\n",
    "\n",
    "W = np.random.normal(loc=0.0, \n",
    "                        scale = np.sqrt(2/(dimension+10)), \n",
    "                        size = (dimension,10))\n",
    "b = 0\n",
    "\n",
    "train_batch = get_mini_batches(train_x, train_y, batch_size)[0] \n",
    "\n",
    "train_batch_x= train_batch[0]\n",
    "train_batch_y= train_batch[1]\n",
    "\n",
    "print(train_batch_x.shape)\n",
    "print(train_batch_y.shape)\n",
    "\n",
    "print(train_batch_y[42])\n",
    "plt.imshow(train_batch_x[42].reshape((28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = np.exp(X)\n",
    "    partition = X_exp.sum(axis=1, keepdims=True)\n",
    "    return X_exp / partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "[Train]  Loss : 0.051149    Acc :  69.2807\n",
      "[Dev]  Loss : 0.003088    Acc :  68.4286\n",
      "[Test]  Loss : 0.006049    Acc :  69.0571\n",
      "\n",
      "Epoch  2\n",
      "[Train]  Loss : 0.041335    Acc :  75.1697\n",
      "[Dev]  Loss : 0.002493    Acc :  74.5714\n",
      "[Test]  Loss : 0.004846    Acc :  75.2000\n",
      "\n",
      "Epoch  3\n",
      "[Train]  Loss : 0.038158    Acc :  77.0151\n",
      "[Dev]  Loss : 0.002294    Acc :  76.4857\n",
      "[Test]  Loss : 0.004466    Acc :  77.0286\n",
      "\n",
      "Epoch  4\n",
      "[Train]  Loss : 0.036721    Acc :  77.8605\n",
      "[Dev]  Loss : 0.002191    Acc :  77.6286\n",
      "[Test]  Loss : 0.004410    Acc :  77.4429\n",
      "\n",
      "Epoch  5\n",
      "[Train]  Loss : 0.035498    Acc :  78.6134\n",
      "[Dev]  Loss : 0.002102    Acc :  78.4286\n",
      "[Test]  Loss : 0.004276    Acc :  78.1857\n",
      "\n",
      "Epoch  6\n",
      "[Train]  Loss : 0.034404    Acc :  79.2555\n",
      "[Dev]  Loss : 0.002103    Acc :  78.5143\n",
      "[Test]  Loss : 0.004147    Acc :  78.7000\n",
      "\n",
      "Epoch  7\n",
      "[Train]  Loss : 0.034543    Acc :  79.1647\n",
      "[Dev]  Loss : 0.002051    Acc :  79.0286\n",
      "[Test]  Loss : 0.004180    Acc :  78.6571\n",
      "\n",
      "Epoch  8\n",
      "[Train]  Loss : 0.033871    Acc :  79.5563\n",
      "[Dev]  Loss : 0.002039    Acc :  79.1143\n",
      "[Test]  Loss : 0.004068    Acc :  79.2286\n",
      "\n",
      "Epoch  9\n",
      "[Train]  Loss : 0.034412    Acc :  79.2151\n",
      "[Dev]  Loss : 0.002068    Acc :  78.8286\n",
      "[Test]  Loss : 0.004207    Acc :  78.4143\n",
      "\n",
      "Epoch  10\n",
      "[Train]  Loss : 0.033840    Acc :  79.5529\n",
      "[Dev]  Loss : 0.002050    Acc :  79.0000\n",
      "[Test]  Loss : 0.004173    Acc :  78.5429\n",
      "\n",
      "Epoch  11\n",
      "[Train]  Loss : 0.033956    Acc :  79.5277\n",
      "[Dev]  Loss : 0.002071    Acc :  78.8000\n",
      "[Test]  Loss : 0.004095    Acc :  78.9000\n",
      "\n",
      "Epoch  12\n",
      "[Train]  Loss : 0.033939    Acc :  79.5429\n",
      "[Dev]  Loss : 0.002067    Acc :  78.8857\n",
      "[Test]  Loss : 0.004188    Acc :  78.5143\n",
      "\n",
      "Epoch  13\n",
      "[Train]  Loss : 0.033309    Acc :  79.8723\n",
      "[Dev]  Loss : 0.002046    Acc :  79.0286\n",
      "[Test]  Loss : 0.004101    Acc :  78.8143\n",
      "\n",
      "Epoch  14\n",
      "[Train]  Loss : 0.033897    Acc :  79.5613\n",
      "[Dev]  Loss : 0.002064    Acc :  78.8571\n",
      "[Test]  Loss : 0.004179    Acc :  78.5000\n",
      "\n",
      "Epoch  15\n",
      "[Train]  Loss : 0.033247    Acc :  79.9529\n",
      "[Dev]  Loss : 0.002038    Acc :  79.1143\n",
      "[Test]  Loss : 0.004084    Acc :  79.0429\n",
      "\n",
      "Epoch  16\n",
      "[Train]  Loss : 0.033489    Acc :  79.7916\n",
      "[Dev]  Loss : 0.002033    Acc :  79.1429\n",
      "[Test]  Loss : 0.004101    Acc :  78.8429\n",
      "\n",
      "Epoch  17\n",
      "[Train]  Loss : 0.033767    Acc :  79.6319\n",
      "[Dev]  Loss : 0.002081    Acc :  78.7143\n",
      "[Test]  Loss : 0.004173    Acc :  78.6571\n",
      "\n",
      "Epoch  18\n",
      "[Train]  Loss : 0.033478    Acc :  79.8000\n",
      "[Dev]  Loss : 0.002033    Acc :  79.1143\n",
      "[Test]  Loss : 0.004098    Acc :  78.9714\n",
      "\n",
      "Early Stopping !\n"
     ]
    }
   ],
   "source": [
    "# train, test\n",
    "\n",
    "dev_temp = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    for j in range(int(max_data/batch_size)):   \n",
    "        \n",
    "        gradient_w =  np.dot(np.transpose(train_batch_x), (softmax(np.dot(train_batch_x,W) + b) - train_batch_y ))/d_size\n",
    "        gradient_b =  np.sum((np.dot(train_batch_x,W) + b - train_batch_y )* 2) /d_size\n",
    "\n",
    "        W -= learning_rate * gradient_w\n",
    "        b -= learning_rate * gradient_b\n",
    "\n",
    "    train_hypothesis = softmax(np.dot(train_x,W) + b)\n",
    "    train_est = np.argmax(train_hypothesis, axis=1) - np.argmax(train_y, axis=1)\n",
    "    train_Loss = np.sum((train_hypothesis - train_y) ** 2) / d_size*0.1\n",
    "    train_acc= len(np.where(train_est==0)[0])/int(d_size*0.85)*100\n",
    "    \n",
    "    dev_hypothesis = softmax(np.dot(dev_x,W) + b)\n",
    "    dev_est = np.argmax(dev_hypothesis, axis=1) - np.argmax(dev_y, axis=1)\n",
    "    dev_Loss = np.sum((dev_hypothesis - dev_y) ** 2) / d_size*0.1\n",
    "    dev_acc = len(np.where(dev_est==0)[0])/int(d_size*0.05)*100\n",
    "    \n",
    "    test_hypothesis = softmax(np.dot(test_x,W) + b)\n",
    "    test_est = np.argmax(test_hypothesis, axis=1) - np.argmax(test_y, axis=1)\n",
    "    test_Loss = np.sum((test_hypothesis - test_y) ** 2) / d_size*0.1\n",
    "    test_acc = len(np.where(test_est==0)[0])/int(d_size*0.1)*100\n",
    "    \n",
    "    \n",
    "    print('Epoch  {:d}'.format(i+1))\n",
    "    print('[Train]  Loss : {:f}    Acc :  {:.4f}'.format(np.sum(train_Loss), train_acc))\n",
    "    print('[Dev]  Loss : {:f}    Acc :  {:.4f}'.format(np.sum(dev_Loss), dev_acc))\n",
    "    print('[Test]  Loss : {:f}    Acc :  {:.4f}'.format(np.sum(test_Loss), test_acc))\n",
    "    print()\n",
    "\n",
    "    if(abs(dev_acc-dev_temp)<0.0001):\n",
    "        print(\"Early Stopping !\")\n",
    "        break\n",
    "    \n",
    "    if((i+1)%5==0):\n",
    "        dev_temp = dev_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Multi-layer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from collections import OrderedDict\n",
    "\n",
    "X, y = datasets.fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =np.expand_dims(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(y)\n",
    "y= enc.transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 섞어주기 위해 셔플\n",
    "\n",
    "tmp = [[a,b] for a, b in zip(X, y)]\n",
    "import random\n",
    "\n",
    "random.shuffle(tmp)\n",
    "\n",
    "X = [n[0] for n in tmp]\n",
    "y = [n[1] for n in tmp]\n",
    "\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_size = 70000 \n",
    "\n",
    "\n",
    "# 학습용데이터\n",
    "train_x = X[0:int(d_size*0.85)]\n",
    "train_y = y[0:int(d_size*0.85)]\n",
    "\n",
    "# dev 데이터\n",
    "dev_x = X[int(d_size*0.85):int(d_size*0.9)]\n",
    "dev_y = y[int(d_size*0.85):int(d_size*0.9)]\n",
    "\n",
    "# test 데이터\n",
    "test_x = X[int(d_size*0.9):d_size]\n",
    "test_y = y[int(d_size*0.9):d_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x = x.astype(np.float32)\n",
    "    x /= 255.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = normalize(train_x)\n",
    "dev_x = normalize(dev_x)\n",
    "test_x = normalize(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59500, 784)\n",
      "(59500, 10)\n",
      "(3500, 784)\n",
      "(3500, 10)\n",
      "(7000, 784)\n",
      "(7000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(dev_x.shape)\n",
    "print(dev_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 가중치와 편향 매개변수의 미분\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 텐서 대응\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t): # negative log likelihood\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실함수\n",
    "        self.y = None    # softmax의 출력\n",
    "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layers_size, weight_init_std = 0.01):\n",
    "        \n",
    "        temp = len(layers_size)-2 # 히든 레이어 갯수\n",
    "        self.temp = temp\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        \n",
    "        self.params['W1'] = weight_init_std * np.random.randn(layers_size[0], layers_size[1])\n",
    "        self.params['b1'] = np.zeros(layers_size[1])\n",
    "        \n",
    "        for i in range(temp):\n",
    "            self.params['W'+str(i+2)] = weight_init_std * np.random.randn(layers_size[i+1], layers_size[i+2])\n",
    "            self.params['b'+str(i+2)] = np.zeros(layers_size[i+2])\n",
    "            \n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        for i in range(temp):\n",
    "            self.layers['Relu'+str(i+1)] = Relu()\n",
    "            self.layers['Affine'+str(i+2)] = Affine(self.params['W'+str(i+2)], self.params['b'+str(i+2)])\n",
    "           \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers: #역전파\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        \n",
    "        for i in range(self.temp):\n",
    "            grads['W'+str(i+2)], grads['b'+str(i+2)] = self.layers['Affine'+str(i+2)].dW, self.layers['Affine'+str(i+2)].db\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정\n",
    "\n",
    "#network = MLP([784,50,10]) # input(784고정) - [hidden] - output(10고정: 10개로 분류)\n",
    "#network = MLP([784,50,50,10])\n",
    "network = MLP([784,50,50,50,10])\n",
    "#network = MLP([784,50,50,50,50,10])\n",
    "\n",
    "iters_num = 10000000\n",
    "train_size = train_x.shape[0]\n",
    "batch_size = 500\n",
    "learning_rate = 0.5\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "max_epoch = 100 # default = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "[Train]  Loss : 2.302369    Acc :  11.1832\n",
      "[Dev]  Loss : 2.302189    Acc :  11.7429\n",
      "[Test]  Loss : 2.302291    Acc :  11.6000\n",
      "\n",
      "Epoch  2\n",
      "[Train]  Loss : 2.301279    Acc :  11.1832\n",
      "[Dev]  Loss : 2.300834    Acc :  11.7429\n",
      "[Test]  Loss : 2.300895    Acc :  11.6000\n",
      "\n",
      "Early Stopping !\n"
     ]
    }
   ],
   "source": [
    "wb_arrange = ['W1', 'b1']\n",
    "tem=1\n",
    "dev_temp=0\n",
    "for i in range(network.temp):\n",
    "    wb_arrange.append('W'+str(i+2))\n",
    "    wb_arrange.append('b'+str(i+2))\n",
    "    \n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = train_x[batch_mask]\n",
    "    t_batch = train_y[batch_mask]\n",
    "    \n",
    "    # backpropa\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # update\n",
    "    for key in wb_arrange:\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(train_x, train_y)\n",
    "        dev_acc = network.accuracy(dev_x, dev_y)\n",
    "        test_acc = network.accuracy(test_x, test_y)\n",
    "        \n",
    "        train_loss = network.loss(train_x, train_y)\n",
    "        dev_loss = network.loss(dev_x, dev_y)\n",
    "        test_loss = network.loss(test_x, test_y)\n",
    "        print('Epoch  {:d}'.format(tem))\n",
    "        print('[Train]  Loss : {:f}    Acc :  {:.4f}'.format(train_loss, train_acc*100))\n",
    "        print('[Dev]  Loss : {:f}    Acc :  {:.4f}'.format(dev_loss, dev_acc*100))\n",
    "        print('[Test]  Loss : {:f}    Acc :  {:.4f}'.format(test_loss, test_acc*100))\n",
    "        print()\n",
    "        \n",
    "        if tem==max_epoch:\n",
    "            break        \n",
    "        tem=tem+1\n",
    "        \n",
    "        \n",
    "        if(abs(dev_acc*100-dev_temp)<0.2):\n",
    "            print(\"Early Stopping !\")\n",
    "            break\n",
    "\n",
    "        dev_temp = dev_acc*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle 이용해 저장\n",
    "import pickle\n",
    "\n",
    "for i in wb_arrange:\n",
    "    with open(\"MLP/MLP_mnist_\"+i+\".pickle\",\"wb\") as fw:\n",
    "        pickle.dump(network.params[i], fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러온 파라미터를 집어넣을 새 모델 생성(layers 의 갯수와 size를 맞게 설정해야된다.)\n",
    "network2 = MLP([784,50,50,50,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle 이용해 불러오기\n",
    "\n",
    "for i in range(network2.temp+1):\n",
    "    with open(\"MLP/MLP_mnist_\"+'W'+str(i+1)+\".pickle\",\"rb\") as fw:\n",
    "        with open(\"MLP/MLP_mnist_\"+'b'+str(i+1)+\".pickle\",\"rb\") as fb:\n",
    "            network2.layers['Affine'+str(i+1)] = Affine(pickle.load(fw), pickle.load(fb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.300895    Acc :  11.6000\n"
     ]
    }
   ],
   "source": [
    "# 불러온 학습된 모델을 통해 테스트(테스트셋에서)\n",
    "\n",
    "acc = network2.accuracy(test_x, test_y)\n",
    "loss = network2.loss(test_x, test_y)\n",
    "\n",
    "print('Loss : {:f}    Acc :  {:.4f}'.format(loss, acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
